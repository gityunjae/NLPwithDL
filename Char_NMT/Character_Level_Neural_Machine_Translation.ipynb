{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175623"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines = pd.read_csv('fra.txt', names = ['src','tar','cc'], sep='\\t')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5694</th>\n",
       "      <td>I was curious.</td>\n",
       "      <td>J'étais curieuse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39275</th>\n",
       "      <td>You're conscientious.</td>\n",
       "      <td>Vous êtes consciencieux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33368</th>\n",
       "      <td>Are you with someone?</td>\n",
       "      <td>Êtes-vous avec quelqu'un ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>I seem to be lost.</td>\n",
       "      <td>Il semble que je sois perdu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36546</th>\n",
       "      <td>It's really horrible.</td>\n",
       "      <td>C'est vraiment horrible.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22395</th>\n",
       "      <td>You're the oldest.</td>\n",
       "      <td>C'est toi le plus vieux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15796</th>\n",
       "      <td>No one asked you.</td>\n",
       "      <td>Personne ne te l'a demandé.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31556</th>\n",
       "      <td>They can't stop you.</td>\n",
       "      <td>Ils ne peuvent pas vous arrêter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47720</th>\n",
       "      <td>I had to get some help.</td>\n",
       "      <td>J'ai dû obtenir de l'aide.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>Is that all of them?</td>\n",
       "      <td>Est-ce là la totalité d'entre elles ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                                    tar\n",
       "5694            I was curious.                      J'étais curieuse.\n",
       "39275    You're conscientious.               Vous êtes consciencieux.\n",
       "33368    Are you with someone?             Êtes-vous avec quelqu'un ?\n",
       "19360       I seem to be lost.           Il semble que je sois perdu.\n",
       "36546    It's really horrible.               C'est vraiment horrible.\n",
       "22395       You're the oldest.               C'est toi le plus vieux.\n",
       "15796        No one asked you.            Personne ne te l'a demandé.\n",
       "31556     They can't stop you.       Ils ne peuvent pas vous arrêter.\n",
       "47720  I had to get some help.             J'ai dû obtenir de l'aide.\n",
       "30287     Is that all of them?  Est-ce là la totalité d'entre elles ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.drop(['cc'], axis = 1)\n",
    "lines = lines[0:60000]\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>I like your tie.</td>\n",
       "      <td>\\t J'aime votre cravate. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23896</th>\n",
       "      <td>I hate goat cheese.</td>\n",
       "      <td>\\t Je déteste le fromage de chèvre. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>Will you go?</td>\n",
       "      <td>\\t Vous y rendrez-vous ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28676</th>\n",
       "      <td>He took off his hat.</td>\n",
       "      <td>\\t Il retira son chapeau. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42593</th>\n",
       "      <td>Is Mary a real blonde?</td>\n",
       "      <td>\\t Est-ce que Marie est une vraie blonde? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22767</th>\n",
       "      <td>Did you find a job?</td>\n",
       "      <td>\\t As-tu trouvé du travail ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35119</th>\n",
       "      <td>I hear Tom hates you.</td>\n",
       "      <td>\\t On m'a dit que Tom te détestait. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57835</th>\n",
       "      <td>We depend on each other.</td>\n",
       "      <td>\\t Nous dépendons les uns des autres. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50453</th>\n",
       "      <td>They hid in the cellar.</td>\n",
       "      <td>\\t Ils se sont cachés dans la cave. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36347</th>\n",
       "      <td>Is this all for real?</td>\n",
       "      <td>\\t Tout ceci est-il réel ? \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                           tar\n",
       "10971          I like your tie.                   \\t J'aime votre cravate. \\n\n",
       "23896       I hate goat cheese.        \\t Je déteste le fromage de chèvre. \\n\n",
       "3063               Will you go?                   \\t Vous y rendrez-vous ? \\n\n",
       "28676      He took off his hat.                  \\t Il retira son chapeau. \\n\n",
       "42593    Is Mary a real blonde?  \\t Est-ce que Marie est une vraie blonde? \\n\n",
       "22767       Did you find a job?               \\t As-tu trouvé du travail ? \\n\n",
       "35119     I hear Tom hates you.        \\t On m'a dit que Tom te détestait. \\n\n",
       "57835  We depend on each other.      \\t Nous dépendons les uns des autres. \\n\n",
       "50453   They hid in the cellar.        \\t Ils se sont cachés dans la cave. \\n\n",
       "36347     Is this all for real?                 \\t Tout ceci est-il réel ? \\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t ' + x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(src_to_index[w])\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t = 0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t > 0 :\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t = t + 1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target= pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더에서 이전 셀의 예측 결과를 다음 단계에 인풋으로 넣는 것이 아니라 이전 셀의 실제 값을 다음 단계에 인풋으로 넣어 학습의 정확도를 높여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq 기계 번역기 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0531 00:20:25.295560 16484 deprecation.py:506] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _=decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0531 00:20:36.895695 16484 deprecation.py:323] From c:\\users\\yunja_kuj61s9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 565s 12ms/sample - loss: 0.7766 - val_loss: 0.6769\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 601s 13ms/sample - loss: 0.4698 - val_loss: 0.5429\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 598s 12ms/sample - loss: 0.3910 - val_loss: 0.4818\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 583s 12ms/sample - loss: 0.3473 - val_loss: 0.4421\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 591s 12ms/sample - loss: 0.3184 - val_loss: 0.4170\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 588s 12ms/sample - loss: 0.2975 - val_loss: 0.4012\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 1065s 22ms/sample - loss: 0.2814 - val_loss: 0.3879\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 585s 12ms/sample - loss: 0.2686 - val_loss: 0.3805\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 588s 12ms/sample - loss: 0.2579 - val_loss: 0.3759\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 592s 12ms/sample - loss: 0.2488 - val_loss: 0.3696\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 587s 12ms/sample - loss: 0.2408 - val_loss: 0.3654\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 587s 12ms/sample - loss: 0.2336 - val_loss: 0.3635\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 583s 12ms/sample - loss: 0.2272 - val_loss: 0.3610\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 591s 12ms/sample - loss: 0.2213 - val_loss: 0.3610\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 584s 12ms/sample - loss: 0.2159 - val_loss: 0.3594\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 583s 12ms/sample - loss: 0.2108 - val_loss: 0.3581\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 577s 12ms/sample - loss: 0.2063 - val_loss: 0.3587\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 580s 12ms/sample - loss: 0.2019 - val_loss: 0.3597\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 577s 12ms/sample - loss: 0.1978 - val_loss: 0.3607\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 581s 12ms/sample - loss: 0.1940 - val_loss: 0.3600\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 575s 12ms/sample - loss: 0.1901 - val_loss: 0.3633\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 569s 12ms/sample - loss: 0.1868 - val_loss: 0.3655\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 557s 12ms/sample - loss: 0.1835 - val_loss: 0.3659\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 561s 12ms/sample - loss: 0.1804 - val_loss: 0.3665\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 558s 12ms/sample - loss: 0.1775 - val_loss: 0.3723\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 588s 12ms/sample - loss: 0.1747 - val_loss: 0.3703\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 563s 12ms/sample - loss: 0.1720 - val_loss: 0.3729\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 558s 12ms/sample - loss: 0.1694 - val_loss: 0.3757\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 577s 12ms/sample - loss: 0.1670 - val_loss: 0.3771\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 570s 12ms/sample - loss: 0.1647 - val_loss: 0.3785\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 571s 12ms/sample - loss: 0.1624 - val_loss: 0.3807\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 573s 12ms/sample - loss: 0.1602 - val_loss: 0.3840\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 571s 12ms/sample - loss: 0.1582 - val_loss: 0.3860\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 571s 12ms/sample - loss: 0.1562 - val_loss: 0.3873\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 572s 12ms/sample - loss: 0.1544 - val_loss: 0.3894\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 566s 12ms/sample - loss: 0.1525 - val_loss: 0.3928\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 571s 12ms/sample - loss: 0.1507 - val_loss: 0.3942\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 568s 12ms/sample - loss: 0.1490 - val_loss: 0.4000\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 569s 12ms/sample - loss: 0.1475 - val_loss: 0.3993\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 567s 12ms/sample - loss: 0.1458 - val_loss: 0.4017\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 569s 12ms/sample - loss: 0.1442 - val_loss: 0.4038\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 565s 12ms/sample - loss: 0.1427 - val_loss: 0.4068\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 567s 12ms/sample - loss: 0.1412 - val_loss: 0.4085\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 562s 12ms/sample - loss: 0.1399 - val_loss: 0.4112\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 568s 12ms/sample - loss: 0.1386 - val_loss: 0.4137\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 567s 12ms/sample - loss: 0.1374 - val_loss: 0.4148\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 563s 12ms/sample - loss: 0.1361 - val_loss: 0.4187\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 567s 12ms/sample - loss: 0.1348 - val_loss: 0.4198\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 563s 12ms/sample - loss: 0.1336 - val_loss: 0.4224\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 564s 12ms/sample - loss: 0.1325 - val_loss: 0.4237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ec0f549d68>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=64, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, \n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs = [decoder_inputs] + decoder_states_inputs, \n",
    "                      outputs = [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i,char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i,char) for char, i in tar_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1,1,tar_vocab_size))\n",
    "    target_seq[0,0,tar_to_index['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        if(sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        target_seq = np.zeros((1,1,tar_vocab_size))\n",
    "        target_seq[0,0,sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Courez ! \n",
      "-----------------------------------\n",
      "입력 문장: I lied.\n",
      "정답 문장:  J'ai menti. \n",
      "번역기가 번역한 문장:  J'ai disposé. \n",
      "-----------------------------------\n",
      "입력 문장: Come in.\n",
      "정답 문장:  Entre. \n",
      "번역기가 번역한 문장:  Entrez ! \n",
      "-----------------------------------\n",
      "입력 문장: I did OK.\n",
      "정답 문장:  Je m'en suis bien sortie. \n",
      "번역기가 번역한 문장:  Je m'en suis bien sortie. \n",
      "-----------------------------------\n",
      "입력 문장: We're sad.\n",
      "정답 문장:  Nous sommes tristes. \n",
      "번역기가 번역한 문장:  Nous sommes en train de se passer. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3, 50, 100, 300, 1001]:\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1])\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: I want a cat.\n",
      "번역기가 번역한 문장:  Je veux du lait. \n"
     ]
    }
   ],
   "source": [
    "sent = ['I want a cat.']\n",
    "input_sent = []\n",
    "for line in sent:\n",
    "    for w in line:\n",
    "        input_sent.append(src_to_index[w])\n",
    "input_sent = pad_sequences([input_sent], maxlen=max_src_len, padding='post')\n",
    "inputSent = to_categorical(input_sent, 79)\n",
    "decoded_sent = decode_sequence(inputSent)\n",
    "print('입력 문장: '+sent[0])\n",
    "print('번역기가 번역한 문장:', decoded_sent[:len(decoded_sent)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
